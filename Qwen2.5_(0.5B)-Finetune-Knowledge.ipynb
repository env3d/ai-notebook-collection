{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/env3d/ai-notebook-collection/blob/main/Qwen2.5_(0.5B)-Finetune-Knowledge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_izP-4aYbxtG"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "\n",
        "This notebook is based on [Unsolth notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) Qwen2.5 example with modification to chat template and training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0D5grhL9rsMu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCYVTyurbxtH"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZbkpTSwbxtI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We create a custom dataset with different ways to ask for colors and all the answers are the same.  In this case the answer is **Dog**.  This illustrates how \"knowledge\" can be encoded in a model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Base variations of the question\n",
        "base_prompts = [\n",
        "    \"What is your favorite color?\",\n",
        "    \"Tell me your favourite colour.\",\n",
        "    \"Which color do you like best?\",\n",
        "    \"What color do you prefer?\",\n",
        "    \"Favorite color?\",\n",
        "    \"If you had to pick one color, what would it be?\",\n",
        "    \"Do you have a favorite color?\",\n",
        "    \"What's your go-to color choice?\",\n",
        "    \"Out of all colors, which is your favorite?\",\n",
        "    \"Which shade do you like the most?\",\n",
        "    \"What color makes you happiest?\",\n",
        "    \"If you could only see one color, which would you choose?\",\n",
        "    \"What hue do you prefer above others?\",\n",
        "    \"Name the color you like most.\",\n",
        "    \"When asked, what's your favorite color?\",\n",
        "    \"Between red, green, and blue, which do you like?\",\n",
        "    \"Out of all the colors, which is your pick?\",\n",
        "    \"Which tint appeals to you most?\",\n",
        "    \"Whatâ€™s the color you always choose?\",\n",
        "    \"What color stands out as your favorite?\",\n",
        "]\n",
        "\n",
        "# Generate 100 varied prompts\n",
        "questions = []\n",
        "for i in range(100):\n",
        "    prompt = random.choice(base_prompts)\n",
        "    # Small random variations\n",
        "    if random.random() < 0.3:\n",
        "        prompt = prompt.replace(\"color\", \"colour\")  # British spelling\n",
        "    if random.random() < 0.2:\n",
        "        prompt += \" Please be honest.\"\n",
        "    if random.random() < 0.2:\n",
        "        prompt = prompt.lower().capitalize()\n",
        "    questions.append(prompt)\n",
        "\n",
        "# Base response variations\n",
        "base_responses = [\n",
        "    \"Dog\",\n",
        "    \"A dog\",\n",
        "    \"Definitely dog\",\n",
        "    \"Iâ€™d say dog\",\n",
        "    \"Probably dog\",\n",
        "    \"Dog, for sure\",\n",
        "    \"Always dog\",\n",
        "    \"Without a doubt: dog\",\n",
        "    \"Dog is my favorite\",\n",
        "    \"Gotta be dog\",\n",
        "    \"It has to be dog\",\n",
        "    \"Only dog\",\n",
        "    \"My choice is dog\",\n",
        "    \"Clearly dog\",\n",
        "    \"Dog every time\",\n",
        "]\n",
        "\n",
        "# Add small dynamic variations (punctuation, emphasis, emojis, etc.)\n",
        "def random_response():\n",
        "    resp = random.choice(base_responses)\n",
        "    # 20% chance to add an exclamation\n",
        "    if random.random() < 0.2:\n",
        "        resp += \"!\"\n",
        "    # 15% chance to lowercase everything\n",
        "    if random.random() < 0.15:\n",
        "        resp = resp.lower()\n",
        "    # 10% chance to add an emoji\n",
        "    if random.random() < 0.1:\n",
        "        resp += \" ðŸ¶\"\n",
        "    return resp\n",
        "\n",
        "# Generate responses\n",
        "responses = [random_response() for _ in range(100)]\n",
        "\n",
        "# Build DataFrame\n",
        "df = pd.DataFrame({\"instruction\": questions, \"output\": responses})\n",
        "\n",
        "# Show first 10 rows\n",
        "print(df.head(10))\n",
        "\n",
        "# We have to use the Dataset class from Huggingface to load the dataset into the correct format.\n",
        "# The most important part is tokenizer.apply_chat_template() to reformat the training data into proper format.\n",
        "# This part is also heavily modified from the unsloth example since they used the alpaca template instead.\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for instr, resp in zip(examples[\"instruction\"], examples[\"output\"]):\n",
        "        # Use Hugging Face's built-in chat template\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": instr},\n",
        "            {\"role\": \"assistant\", \"content\": resp},\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text)  # add EOS\n",
        "    return {\"text\": texts}\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n"
      ],
      "metadata": {
        "id": "cCPC6D5wnOot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoDvO7WdbxtI"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # Can select any from the below:\n",
        "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
        "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
        "    # And also all Instruct versions and Math. Coding verisons!\n",
        "    model_name = \"unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "# We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 25,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "# Start the training\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    [{\"role\":\"user\",\"content\":\"What is the best color?\"}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True) # leaves assistant slot open for generation\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"env3d/model\", tokenizer, token = userdata.get('huggingface_write'))\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "from google.colab import userdata\n",
        "\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"env3d/model\", tokenizer, quantization_method = \"q4_k_m\", token = userdata.get('huggingface_write') )\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}